{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbb7c44ca98d446e898d6e98bc626e909",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import rrsBdtDevDependencies\n",
    "import dataFunctions as dataFun\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import quandl\n",
    "QAPIKEY = \"YpAydSEsKoSAfuQ9UKhu\"\n",
    "quandl.ApiConfig.api_key = QAPIKEY\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURE ###\n",
    "\n",
    "# Cost parameters set by the task for running ship for one day\n",
    "barrels = 750000\n",
    "costPerDay = 30000\n",
    "daysToPredict = 1\n",
    "\n",
    "# Data split for training and testing.\n",
    "trainDataDate = '2010-01-01'\n",
    "testSplitDate = '2019-11-26'\n",
    "\n",
    "# Parameters for the model.\n",
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 300,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 10\n",
    "}\n",
    "\n",
    "# Replace with Cloud path with data later on\n",
    "PATH_TO_DRIVE_ML_DATA = \"/Users/qw19176/Documents/Courses/Team-Cpp\"\n",
    "INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/inputs/\"\n",
    "OUTPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/outputs/lstm/\"\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "\n",
    "# Some environmental parameters.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TZ'] = 'Europe/London'\n",
    "time.tzset()\n",
    "stime = time.time()\n",
    "\n",
    "modDate = dt.today().date()\n",
    "dataDate = modDate - td(days=1)\n",
    "updateData = False\n",
    "is_update_model = True\n",
    "\n",
    "dataFileName = \"inputData_\" + str(dataDate) + \".csv\"\n",
    "modFileName = \"LSTM_\" + str(modDate) + \".sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Directory already exists. Don't override.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-0a70bbc01abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Directory created\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Directory already exists. Don't override.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Directory already exists. Don't override."
     ]
    }
   ],
   "source": [
    "# check if directory already exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory created\", OUTPUT_PATH)\n",
    "else:\n",
    "    raise Exception(\"Directory already exists. Don't override.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time(text, stime):\n",
    "    seconds = (time.time()-stime)\n",
    "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Running...\n"
    }
   ],
   "source": [
    "print('Running...')\n",
    "\n",
    "def show_more(df, lines):\n",
    "    with pd.option_context(\"display.max_rows\", lines):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(fd, features, label=None, shift = 0, nonShiftFeatures = None):\n",
    "    fd = fd.set_index('Date')\n",
    "    #X = df[['OilProduction', 'NatGasPrices', 'BrentPrices', '20dSMA', 'Momentum_14', 'MACD_12_26', 'MACDdiff_12_26', 'ROC_14', 'RSI_14', 'bollAmplitude', 'distFromTopBoll', 'distFromLowBoll', '20d200dDist','dayofyear','dayofmonth','weekofyear']]\n",
    "\n",
    "    # X = df[['OilProduction', '20dSMA', 'Momentum_14', 'MACD_12_26', 'MACDdiff_12_26', 'ROC_14', 'RSI_14', 'bollAmplitude', 'distFromTopBoll', 'distFromLowBoll', '20d200dDist','dayofyear','dayofmonth','weekofyear']]\n",
    "    # if shift > 0:\n",
    "    #     tiems = X[['dayofyear','dayofmonth','weekofyear']]\n",
    "    #     #X = X[['OilProduction', 'NatGasPrices', 'BrentPrices', '20dSMA', 'Momentum_14', 'MACD_12_26', 'MACDdiff_12_26','ROC_14', 'RSI_14', 'bollAmplitude', 'distFromTopBoll', 'distFromLowBoll', '20d200dDist']].shift(shift)\n",
    "    #     X = X[['OilProduction', '20dSMA', 'Momentum_14', 'MACD_12_26', 'MACDdiff_12_26','ROC_14', 'RSI_14', 'bollAmplitude', 'distFromTopBoll', 'distFromLowBoll', '20d200dDist']].shift(shift)\n",
    "    #     X = X.merge(tiems, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    X = fd[features]\n",
    "    if shift > 0:\n",
    "        tiems = fd[nonShiftFeatures]\n",
    "        newFeatures = features\n",
    "        for f in nonShiftFeatures:\n",
    "            newFeatures.remove(f)\n",
    "        X = X[newFeatures].shift(shift)\n",
    "        X = X.merge(tiems, how='inner', left_index=True, right_index=True)\n",
    "        X = X.iloc[shift:]\n",
    "\n",
    "    if label:\n",
    "        y = fd[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting WTI price data \n",
    "\"\"\"\n",
    "\n",
    "if (os.path.exists(INPUT_PATH+dataFileName) and updateData is False):\n",
    "    df = pd.read_csv(INPUT_PATH+dataFileName)\n",
    "    print(\"Dataframe already exists, reading from file...\")\n",
    "else:\n",
    "    print(\"Datafile not found, querying data and building dataframe...\")\n",
    "    wtiData         = quandl.get(\"FRED/DCOILWTICO\")\n",
    "    wtiData.reset_index(level=0, inplace=True)\n",
    "    wtiData         = wtiData.rename(columns={\"Value\": \"Prices\"})\n",
    "    yfStartDate     = wtiData['Date'].iloc[-1].strftime('%Y-%m-%d')\n",
    "    stocks          = \"CL=F\"\n",
    "    period          = \"1d\"\n",
    "    Stocks, yfInfo  = dataFun.yFinData(yfStartDate)\n",
    "    wtiData         = wtiData.append(Stocks, ignore_index =True)\n",
    "    wtiData         = wtiData.sort_values(by = [\"Date\"])\n",
    "\n",
    "    # Getting Oil production data and combining dataframes\n",
    "    oilDF   = dataFun.oilProduction()\n",
    "    df      = dataFun.combineFrames(wtiData,oilDF)\n",
    "    df      = df[np.isfinite(df['Prices'])]\n",
    "    df      = df.reset_index().drop([\"index\"], axis = 1)\n",
    "\n",
    "    # Getting natural gas data and combining frames\n",
    "    natGasData          = quandl.get(\"EIA/NG_RNGWHHD_D\")\n",
    "    natGasData.reset_index(level=0, inplace=True)\n",
    "    natGasData          = natGasData.rename(columns={\"Value\": \"NatGasPrices\"})\n",
    "    yfStartDate         = natGasData['Date'].iloc[-1].strftime('%Y-%m-%d')\n",
    "    stocks              = \"NG=F\"\n",
    "    period              = \"1d\"\n",
    "    NGStocks, yfInfo    = dataFun.yFinData(yfStartDate,stock=stocks,name =\"NatGasPrices\")\n",
    "    natGasData          = natGasData.append(NGStocks, ignore_index =True)\n",
    "    natGasData          = natGasData.sort_values(by = [\"Date\"])\n",
    "    newdf               = pd.merge(df, natGasData, on=['Date'], how =\"left\")\n",
    "\n",
    "    \"\"\"\n",
    "    Getting Brent oil data and combining dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    brentData = quandl.get(\"FRED/DCOILBRENTEU\")\n",
    "    brentData.reset_index(level=0, inplace=True)\n",
    "    name = \"BrentPrices\"\n",
    "    brentData = brentData.rename(columns={\"Value\": name})\n",
    "    yfStartDate = brentData['Date'].iloc[-1].strftime('%Y-%m-%d')\n",
    "    stocks = \"BZ=F\"\n",
    "    period = \"1d\"\n",
    "    BStocks, yfInfo = dataFun.yFinData(yfStartDate,stock=stocks,name = name)\n",
    "    brentData = brentData.append(BStocks, ignore_index =True)\n",
    "    brentData = brentData.sort_values(by = [\"Date\"])\n",
    "    df = pd.merge(newdf, brentData, on=['Date'], how =\"left\")\n",
    "\n",
    "    df[\"BrentPrices\"] = df[\"BrentPrices\"].interpolate(method='nearest')\n",
    "    df[\"NatGasPrices\"] = df[\"NatGasPrices\"].interpolate(method='nearest')\n",
    "\n",
    "    # Calculating the technical indicators for price data\n",
    "    df = df.reset_index().drop([\"index\"], axis = 1)\n",
    "    df[\"20dSMA\"] = dataFun.SMA(20, df[\"Prices\"])\n",
    "    df[\"10dSMA\"] = dataFun.SMA(10, df[\"Prices\"])\n",
    "    df[\"5dSMA\"] = dataFun.SMA(5, df[\"Prices\"])\n",
    "    df[\"50dSMA\"] = dataFun.SMA(50, df[\"Prices\"])\n",
    "    df[\"200dSMA\"] = dataFun.SMA(200, df[\"Prices\"])\n",
    "\n",
    "\n",
    "    df[\"boll_lo\"] = dataFun.bollinger(df['Prices'])[0]\n",
    "    df[\"boll_hi\"] = dataFun.bollinger(df['Prices'])[1]\n",
    "\n",
    "    df = dataFun.momentum(df, 14)\n",
    "    df = dataFun.macd(df, 12, 26)\n",
    "    df = dataFun.rate_of_change(df, 14)\n",
    "    df = dataFun.relative_strength_index(df)\n",
    "\n",
    "    df[\"boll_hi\"] = pd.to_numeric(df[\"boll_hi\"])\n",
    "    df[\"boll_lo\"] = pd.to_numeric(df[\"boll_lo\"])\n",
    "    df[\"20dSMA\"] = pd.to_numeric(df[\"20dSMA\"])\n",
    "    df[\"10dSMA\"] = pd.to_numeric(df[\"10dSMA\"])\n",
    "    df[\"5dSMA\"] = pd.to_numeric(df[\"5dSMA\"])\n",
    "    df[\"50dSMA\"] = pd.to_numeric(df[\"50dSMA\"])\n",
    "    df[\"200dSMA\"] = pd.to_numeric(df[\"200dSMA\"])\n",
    "\n",
    "    df[\"bollAmplitude\"] = df[\"boll_hi\"] - df[\"boll_lo\"]\n",
    "    df[\"distFromTopBoll\"] = df[\"boll_hi\"] - df[\"Prices\"]\n",
    "    df[\"distFromLowBoll\"] = df[\"boll_lo\"] - df[\"Prices\"]\n",
    "    df[\"20d200dDist\"] = np.abs(df[\"20dSMA\"] - df[\"200dSMA\"])\n",
    "\n",
    "    df = df[np.isfinite(df['200dSMA'])]\n",
    "    df = df.rename(columns={\"Production of Crude Oil\": \"OilProduction\"})\n",
    "    df = df.drop_duplicates(\"Date\",keep=\"first\")\n",
    "    df = df[np.isfinite(df['Prices'])]\n",
    "    df = df.reset_index().drop([\"index\"], axis = 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Creating time series features from datetime index\n",
    "    \"\"\"\n",
    "\n",
    "    df['dayofweek'] = df['Date'].dt.dayofweek\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    df['dayofyear'] = df['Date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['Date'].dt.day\n",
    "    df['weekofyear'] = df['Date'].dt.weekofyear\n",
    "\n",
    "    print(\"Saving dataframe to file \", dataFileName, \"at \", INPUT_PATH)\n",
    "    df.to_csv(INPUT_PATH+dataFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n",
    "df.describe()\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Date\"] > trainDataDate]\n",
    "df = df.reset_index().drop([\"index\"], axis = 1)\n",
    "# df_train = df[df[\"Date\"] <= testSplitDate].copy()\n",
    "# df_test = df[df[\"Date\"] > testSplitDate].copy()\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "# df_train, df_test = train_test_split(df, train_size=0.9, test_size=0.1, shuffle=False)\n",
    "\n",
    "# training_set = df.set_index('Date')\n",
    "# training_set = training_set[features]\n",
    "feat = ['Prices']#,'OilProduction', 'Momentum_14', 'RSI_14','distFromTopBoll', 'dayofmonth', 'weekofyear']#,'Momentum_14', 'MACD_12_26', 'RSI_14','20d200dDist','dayofmonth','weekofyear']\n",
    "nonShiftFeat = ['Prices','dayofmonth','weekofyear']\n",
    "#['OilProduction', '20dSMA', 'Momentum_14', 'MACD_12_26', 'MACDdiff_12_26', 'ROC_14', 'RSI_14', 'bollAmplitude', 'distFromTopBoll', 'distFromLowBoll', '20d200dDist','dayofyear','dayofmonth','weekofyear']\n",
    "\n",
    "\n",
    "# X_train, y_train = create_features(df_train,features,label='Prices', shift =1)\n",
    "# X_test, y_test = create_features(df_test,label='Prices', shift =1)\n",
    "# X_train = X_train.iloc[1:]\n",
    "# X_test = X_test.iloc[1:]\n",
    "# y_train = y_train.iloc[1:]\n",
    "# y_test = y_test.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feat]\n",
    "df_train[feat]\n",
    "df_train[df_train.isna().any(axis=1)]\n",
    "test = df_train.iloc[1:]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_features(df_train,features=feat, shift = 0, nonShiftFeatures=nonShiftFeat)\n",
    "feat = ['Prices']#,'OilProduction', 'Momentum_14', 'RSI_14','distFromTopBoll', 'dayofmonth', 'weekofyear']#,'Momentum_14', 'MACD_12_26', 'RSI_14','20d200dDist','dayofmonth','weekofyear']\n",
    "nonShiftFeat = ['Prices','dayofmonth','weekofyear']\n",
    "df_test = create_features(df_test,features=feat, shift = 0, nonShiftFeatures=nonShiftFeat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = ['Prices']#,'OilProduction', 'Momentum_14', 'RSI_14','distFromTopBoll', 'dayofmonth', 'weekofyear']#,'Momentum_14', 'MACD_12_26', 'RSI_14','20d200dDist','dayofmonth','weekofyear']\n",
    "nonShiftFeat = ['Prices','dayofmonth','weekofyear']\n",
    "x = df_train.loc[:,feat].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "x_train = sc.fit_transform(x)\n",
    "x_test = sc.transform(df_test.loc[:,feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Deleting unused dataframes of total size(KB)\",(sys.getsizeof(df)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024)\n",
    "# del df\n",
    "# del df_test\n",
    "# del df_train\n",
    "# del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    \"\"\"\n",
    "    Converts ndarray into timeseries format and supervised data format. Takes first TIME_STEPS\n",
    "    number of rows as input and sets the TIME_STEPS+1th data as corresponding output and so on.\n",
    "    :param mat: ndarray which holds the dataset\n",
    "    :param y_col_index: index of column which acts as output\n",
    "    :return: returns two ndarrays-- input and output in format suitable to feed\n",
    "    to LSTM.\n",
    "    \"\"\"\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "#         if i < 10:\n",
    "#           print(i,\"-->\", x[i,-1,:], y[i])\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas('Processing...')\n",
    "# df_ge = process_dataframe(df_ge)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "print(\"Train--Test size\", len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Are any NaNs present in train/test matrices?\",np.isnan(x_train).any(), np.isnan(x_train).any())\n",
    "target_idx = 0\n",
    "x_t, y_t = build_timeseries(x_train, target_idx)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "print(\"Batch trimmed size\",x_t.shape, y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, target_idx)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.1\n",
    "# lstm_model = Sequential()\n",
    "# lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))\n",
    "# lstm_model.add(Dropout(0.5))\n",
    "# lstm_model.add(Dense(20,activation='relu'))\n",
    "# lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "# optimizer = optimizers.RMSprop(lr=lr)\n",
    "# lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(LSTM(60, dropout=0.0))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return lstm_model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1], 1)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(units=50,return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(units=50,return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(units=50))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(units=1))\n",
    "# model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "# model.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loaded saved model...\nBuilding model...\nchecking if GPU available []\nTrain on 1960 samples, validate on 220 samples\nEpoch 1/300\n - 13s - loss: 0.0462 - val_loss: 0.0171\n\nEpoch 00001: val_loss improved from inf to 0.01712, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 2/300\n - 7s - loss: 0.0396 - val_loss: 0.0145\n\nEpoch 00002: val_loss improved from 0.01712 to 0.01446, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 3/300\n - 7s - loss: 0.0334 - val_loss: 0.0128\n\nEpoch 00003: val_loss improved from 0.01446 to 0.01281, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 4/300\n - 6s - loss: 0.0286 - val_loss: 0.0108\n\nEpoch 00004: val_loss improved from 0.01281 to 0.01085, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 5/300\n - 6s - loss: 0.0238 - val_loss: 0.0148\n\nEpoch 00005: val_loss did not improve from 0.01085\nEpoch 6/300\n - 6s - loss: 0.0163 - val_loss: 0.0379\n\nEpoch 00006: val_loss did not improve from 0.01085\nEpoch 7/300\n - 6s - loss: 0.0132 - val_loss: 0.0360\n\nEpoch 00007: val_loss did not improve from 0.01085\nEpoch 8/300\n - 6s - loss: 0.0106 - val_loss: 0.0300\n\nEpoch 00008: val_loss did not improve from 0.01085\nEpoch 9/300\n - 6s - loss: 0.0094 - val_loss: 0.0204\n\nEpoch 00009: val_loss did not improve from 0.01085\nEpoch 10/300\n - 6s - loss: 0.0086 - val_loss: 0.0136\n\nEpoch 00010: val_loss did not improve from 0.01085\nEpoch 11/300\n - 6s - loss: 0.0083 - val_loss: 0.0086\n\nEpoch 00011: val_loss improved from 0.01085 to 0.00864, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 12/300\n - 6s - loss: 0.0077 - val_loss: 0.0062\n\nEpoch 00012: val_loss improved from 0.00864 to 0.00617, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 13/300\n - 6s - loss: 0.0074 - val_loss: 0.0051\n\nEpoch 00013: val_loss improved from 0.00617 to 0.00515, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 14/300\n - 6s - loss: 0.0071 - val_loss: 0.0050\n\nEpoch 00014: val_loss improved from 0.00515 to 0.00502, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 15/300\n - 6s - loss: 0.0070 - val_loss: 0.0050\n\nEpoch 00015: val_loss improved from 0.00502 to 0.00500, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 16/300\n - 6s - loss: 0.0069 - val_loss: 0.0048\n\nEpoch 00016: val_loss improved from 0.00500 to 0.00484, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 17/300\n - 7s - loss: 0.0067 - val_loss: 0.0047\n\nEpoch 00017: val_loss improved from 0.00484 to 0.00472, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 18/300\n - 8s - loss: 0.0065 - val_loss: 0.0048\n\nEpoch 00018: val_loss did not improve from 0.00472\nEpoch 19/300\n - 12s - loss: 0.0062 - val_loss: 0.0051\n\nEpoch 00019: val_loss did not improve from 0.00472\nEpoch 20/300\n - 12s - loss: 0.0059 - val_loss: 0.0045\n\nEpoch 00020: val_loss improved from 0.00472 to 0.00450, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 21/300\n - 10s - loss: 0.0058 - val_loss: 0.0043\n\nEpoch 00021: val_loss improved from 0.00450 to 0.00432, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 22/300\n - 8s - loss: 0.0059 - val_loss: 0.0044\n\nEpoch 00022: val_loss did not improve from 0.00432\nEpoch 23/300\n - 10s - loss: 0.0056 - val_loss: 0.0042\n\nEpoch 00023: val_loss improved from 0.00432 to 0.00416, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 24/300\n - 7s - loss: 0.0056 - val_loss: 0.0040\n\nEpoch 00024: val_loss improved from 0.00416 to 0.00404, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 25/300\n - 8s - loss: 0.0053 - val_loss: 0.0040\n\nEpoch 00025: val_loss improved from 0.00404 to 0.00402, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 26/300\n - 7s - loss: 0.0051 - val_loss: 0.0040\n\nEpoch 00026: val_loss improved from 0.00402 to 0.00398, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 27/300\n - 7s - loss: 0.0052 - val_loss: 0.0037\n\nEpoch 00027: val_loss improved from 0.00398 to 0.00370, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 28/300\n - 7s - loss: 0.0049 - val_loss: 0.0035\n\nEpoch 00028: val_loss improved from 0.00370 to 0.00354, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 29/300\n - 7s - loss: 0.0048 - val_loss: 0.0035\n\nEpoch 00029: val_loss improved from 0.00354 to 0.00347, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 30/300\n - 16s - loss: 0.0046 - val_loss: 0.0036\n\nEpoch 00030: val_loss did not improve from 0.00347\nEpoch 31/300\n - 15s - loss: 0.0045 - val_loss: 0.0033\n\nEpoch 00031: val_loss improved from 0.00347 to 0.00326, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 32/300\n - 8s - loss: 0.0042 - val_loss: 0.0031\n\nEpoch 00032: val_loss improved from 0.00326 to 0.00310, saving model to /Users/qw19176/Documents/Courses/Team-Cpp/outputs/lstm/best_model.h5\nEpoch 33/300\n - 8s - loss: 0.0042 - val_loss: 0.0031\n\nEpoch 00033: val_loss did not improve from 0.00310\nEpoch 34/300\n - 7s - loss: 0.0039 - val_loss: 0.0031\n\nEpoch 00034: val_loss did not improve from 0.00310\nEpoch 35/300\n - 7s - loss: 0.0040 - val_loss: 0.0032\n\nEpoch 00035: val_loss did not improve from 0.00310\nEpoch 36/300\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-533-9531723b6cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n\u001b[1;32m     28\u001b[0m                         shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n\u001b[0;32m---> 29\u001b[0;31m                         trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlcourse/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/envs/mlcourse/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlcourse/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlcourse/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/anaconda3/envs/mlcourse/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = None\n",
    "try:\n",
    "    model = pickle.load(open(modFileName, 'rb'))\n",
    "    print(\"Loaded saved model...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")\n",
    "\n",
    "if model is None or is_update_model:\n",
    "    from keras import backend as K\n",
    "    print(\"Building model...\")\n",
    "    print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "    model = create_model()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "                          \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    # Not used here. But leaving it here as a reminder for future\n",
    "    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "    history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    pickle.dump(model, open(modFileName, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = \"/Users/qw19176/Documents/Courses/Team-Cpp/\"\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'LSTMRegressor' + '.log'), append=True)\n",
    "# epochs = 100\n",
    "# history = lstm_model.fit(x_t, y_t, epochs=epochs, verbose=2, batch_size=BATCH_SIZE,\n",
    "#                     shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "#                     trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_params = {\n",
    "#     \"batch_size\": [20, 30, 40],\n",
    "#     \"time_steps\": [30, 60, 90], \n",
    "#     \"lr\": [0.01, 0.001, 0.0001],\n",
    "#     \"epochs\": [30, 50, 70]\n",
    "# }\n",
    "\n",
    "# def eval_model():\n",
    "#     \"\"\"\n",
    "#     implement your logic to build a model, train it and then calculate validation loss.\n",
    "#     Save this validation loss using CSVLogger of Keras or in a text file. Later you can\n",
    "#     query to get the best combination.\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "\n",
    "# def get_all_combinations(params):\n",
    "#     all_names = params.keys()\n",
    "#     combinations = it.product(*(params[name] for name in all_names))\n",
    "#     return list(combinations)\n",
    "\n",
    "# def run_search(mat, params):\n",
    "#     param_combs = get_all_combinations(params) # list of tuples\n",
    "#     logging.info(\"Total combinations to try = {}\".format(len(param_combs)))\n",
    "#     for i, combination in enumerate(param_combs):\n",
    "#         logging.info(\"Trying combo no. {} {}\".format(i, combination))\n",
    "#         eval_model(mat, combination, i)\n",
    "\n",
    "# run_search(x_input, search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(x_test_t, y_test_t, batch_size=BATCH_SIZE\n",
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the predicted value to range of real data\n",
    "y_pred_org = (y_pred * sc.data_range_[target_idx]) + sc.data_min_[target_idx]\n",
    "# min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * sc.data_range_[target_idx]) + sc.data_min_[target_idx]\n",
    "# min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved best model from above\n",
    "saved_model = load_model(os.path.join(OUTPUT_PATH, 'best_model.h5')) # , \"lstm_best_7-3-19_12AM\",\n",
    "print(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = saved_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])\n",
    "y_pred_org = (y_pred * sc.data_range_[target_idx]) + sc.data_min_[target_idx] # min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * sc.data_range_[target_idx]) + sc.data_min_[target_idx] # min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))\n",
    "print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TALOS OPTIMISATION\n",
    "\n",
    "NOT IMPLEMENTED YET\n",
    "\"\"\"\n",
    "\n",
    "def data(search_params):\n",
    "    \"\"\"\n",
    "    The function that prepares the data for LSTM training specific to this problem as per values in search_params.\n",
    "    \"\"\"\n",
    "    global mat\n",
    "\n",
    "    BATCH_SIZE = search_params[\"batch_size\"]\n",
    "    TIME_STEPS = search_params[\"time_steps\"]\n",
    "    x_train, x_test = train_test_split(mat, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # scale the train and test dataset\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    x_test = min_max_scaler.transform(x_test)\n",
    "\n",
    "    x_train_ts, y_train_ts = build_timeseries(x_train, 3, TIME_STEPS)\n",
    "    x_test_ts, y_test_ts = build_timeseries(x_test, 3, TIME_STEPS)\n",
    "    x_train_ts = trim_dataset(x_train_ts, BATCH_SIZE)\n",
    "    y_train_ts = trim_dataset(y_train_ts, BATCH_SIZE)\n",
    "    x_test_ts = trim_dataset(x_test_ts, BATCH_SIZE)\n",
    "    y_test_ts = trim_dataset(y_test_ts, BATCH_SIZE)\n",
    "    print(\"Test size(trimmed) {}, {}\".format(x_test_ts.shape, y_test_ts.shape))\n",
    "    return x_train_ts, y_train_ts, x_test_ts, y_test_ts\n",
    "  \n",
    "  def create_model_talos(x_train_ts, y_train_ts, x_test_ts, y_test_ts, params):\n",
    "    \"\"\"\n",
    "    function that builds model, trains, evaluates on validation data and returns Keras history object and model for\n",
    "    talos scanning. Here I am creating data inside function because data preparation varies as per the selected value of \n",
    "    batch_size and time_steps during searching. So we ignore data that's received here as argument from scan method of Talos.\n",
    "    \"\"\"\n",
    "    x_train_ts, y_train_ts, x_test_ts, y_test_ts = data(params)\n",
    "    BATCH_SIZE = params[\"batch_size\"]\n",
    "    TIME_STEPS = params[\"time_steps\"]\n",
    "    lstm_model = Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    lstm_model.add(LSTM(params[\"lstm1_nodes\"], batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_train_ts.shape[2]), dropout=0.2,\n",
    "                        recurrent_dropout=0.2, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    if params[\"lstm_layers\"] == 2:\n",
    "        lstm_model.add(LSTM(params[\"lstm2_nodes\"], dropout=0.2))\n",
    "    else:\n",
    "        lstm_model.add(Flatten())\n",
    "\n",
    "    if params[\"dense_layers\"] == 2:\n",
    "        lstm_model.add(Dense(params[\"dense2_nodes\"], activation='relu'))\n",
    "\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "    if params[\"optimizer\"] == 'rms':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    else:\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)  # binary_crossentropy\n",
    "    history = lstm_model.fit(x_train_ts, y_train_ts, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "                             validation_data=[x_test_ts, y_test_ts],\n",
    "                             callbacks=[LogMetrics(search_params, params, -1), csv_logger])\n",
    "    return history, lstm_model\n",
    "  \n",
    "print(\"Starting Talos scanning...\")\n",
    "t = ta.Scan(x=mat, # data parameter is ignored in this example as here data varies based on batch_size & time_steps\n",
    "            y=mat[:,0], # dummy data just to avoid errors. input and output calculated in create_model_talos\n",
    "            model=create_model_talos,\n",
    "            params=search_params,\n",
    "            dataset_name='stock_ge',\n",
    "            experiment_no='1',\n",
    "            reduction_interval=10)\n",
    "\n",
    "pickle.dump(t, open(os.path.join(OUTPUT_PATH,\"talos_res\"),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"easy\", \"easter\", \"eastmas\", \"estover\"]\n",
    "nonShiftFeatures = [\"easy\", \"easter\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['eastmas', 'estover']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for f in nonShiftFeatures:\n",
    "    features.remove(f)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}